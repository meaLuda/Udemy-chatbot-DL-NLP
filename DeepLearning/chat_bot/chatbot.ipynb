{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatBot Tutoriol: Udemy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ---\n",
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "import re \n",
    "import time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ---\n",
    "###  1. DATA LOADING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lines\n",
    "lines = open('data/movie_lines.txt',encoding='utf-8',\\\n",
    "    errors='ignore').read().split('\\n')\n",
    "\n",
    "# conversations\n",
    "conversations = open('data/movie_conversations.txt',\\\n",
    "    encoding='utf-8',errors='ignore').read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['L1045', 'u0', 'm0', 'BIANCA', 'They do not!']\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "a_line = \"L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!\"\n",
    "print(a_line.split(\" +++$+++ \"))\n",
    "print(len(a_line.split(\" +++$+++ \")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. DATA CLEANING | PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ---\n",
    "### Creating a dictionary to map line and id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "id_to_line = {}\n",
    "\n",
    "for line in lines:\n",
    "    _line = line.split(' +++$+++ ')\n",
    "    if len(_line) == 5:\n",
    "        id_to_line[_line[0]] = _line[4]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(id_to_line))\n",
    "# id_to_line.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ---\n",
    "### Creating a list of the whole conversation \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['u0', 'u2', 'm0', \"['L194', 'L195', 'L196', 'L197']\"]\n",
      "['L194', 'L195', 'L196', 'L197']\n",
      "'L194', 'L195', 'L196', 'L197'\n",
      "L194, L195, L196, L197\n",
      "L194,L195,L196,L197\n"
     ]
    }
   ],
   "source": [
    "convo_test = \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L194', 'L195', 'L196', 'L197']\"\n",
    "\n",
    "\n",
    "print(convo_test.split(\" +++$+++ \"))\n",
    "print(convo_test.split(\" +++$+++ \")[-1])\n",
    "\n",
    "print(convo_test.split(\" +++$+++ \")[-1][1:-1])\n",
    "print(convo_test.split(\" +++$+++ \")[-1][1:-1].replace(\"'\",\"\"))\n",
    "print(convo_test.split(\" +++$+++ \")[-1][1:-1].replace(\"'\",\"\").replace(\" \", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversations_id = []\n",
    "\n",
    "\n",
    "for convo in conversations[:-1]:\n",
    "    _convers =  convo.split(\" +++$+++ \")[-1][1:-1].replace(\"'\",\"\").replace(\" \", \"\")\n",
    "    conversations_id.append(_convers.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['L194', 'L195', 'L196', 'L197'],\n",
       " ['L198', 'L199'],\n",
       " ['L200', 'L201', 'L202', 'L203'],\n",
       " ['L204', 'L205', 'L206'],\n",
       " ['L207', 'L208'],\n",
       " ['L271', 'L272', 'L273', 'L274', 'L275'],\n",
       " ['L276', 'L277'],\n",
       " ['L280', 'L281'],\n",
       " ['L363', 'L364'],\n",
       " ['L365', 'L366']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversations_id[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ---\n",
    "### Getting separetly the questions and answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "answers = []\n",
    "\n",
    "# mapping ids\n",
    "\n",
    "for c in conversations_id:\n",
    "    for i in range(len(c) -1): # dont go above upper bound\n",
    "        questions.append(id_to_line[c[i]]) # get first key value in the conversations_id\n",
    "        answers.append(id_to_line[c[i+1]]) # get the next item as answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.', \"Well, I thought we'd start with pronunciation, if that's okay with you.\"]\n",
      "[\"Well, I thought we'd start with pronunciation, if that's okay with you.\", 'Not the hacking and gagging and spitting part.  Please.']\n"
     ]
    }
   ],
   "source": [
    "print(questions[:2])\n",
    "print(answers[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ---\n",
    "### Text cleaning 101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# func: clean text and modify it \n",
    "# (to_lowercase, remove apostraphe)\n",
    "\n",
    "test_txt = \"\"\"\n",
    "Well, I thought we'd start with pronunciation, she's he's\n",
    "if that's okay with you.,Not the hacking and gagging and spitting part. Please.\n",
    "\"\"\"\n",
    "\n",
    "def clean_text(text):\n",
    "    # to lowercase\n",
    "    _text = text.lower()\n",
    "    _text = re.sub(r\"i'm\",\"i am\", _text)\n",
    "    _text = re.sub(r\"he's\",\"he is\", _text)\n",
    "    _text = re.sub(r\"she's\",\"she is\", _text)\n",
    "    _text = re.sub(r\"\\'li\",\"will\", _text)\n",
    "    _text = re.sub(r\"that's\",\"that is\", _text)\n",
    "    _text = re.sub(r\"we'd\",\"we would\", _text)\n",
    "    _text = re.sub(r\"[-()\\\"#/@;:<>{}=-|.?,]\",\"\", _text) # remove non-text symbol\n",
    "    \n",
    "    final_txt = _text\n",
    "\n",
    "\n",
    "    return final_txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "           \n",
      "             \n",
      "\n"
     ]
    }
   ],
   "source": [
    " # clearn text test\n",
    "print(clean_text(test_txt))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b102dd91821e75c9f88fc330d6715d23f099a1db61992b7545bc720b1ce0384a"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('tensor_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
